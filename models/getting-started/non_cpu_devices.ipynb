{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The CPU\n",
        "\n",
        "We all know that the cpu is the heart of a computer. However, it actually isn't the best for training models. Pytorch allows us to utilize our GPU's ability to compute many parallel computations at once to speed up training. So, how do we do this?"
      ],
      "metadata": {
        "id": "uFULGEbTufhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA\n",
        "\n",
        "CUDA is the most well known toolkit used to speed up training. It requires an Nvidia GPU.\n",
        "\n",
        "Utilizing CUDA is extremely simple with Pytorch. Let's look at an example below."
      ],
      "metadata": {
        "id": "P3sNkwjovB7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GJYONsGuZQU",
        "outputId": "d2bd253a-c902-4111-a85e-506bea06a6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# import necessary module\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MPS\n",
        "\n",
        "MPS is Apple silicon's equivalent to CUDA. What's nice about MPS is that it can utilize your entire device's ram, rather than being limited by a standard GPU's vram.\n",
        "\n",
        "It is activated just like CUDA."
      ],
      "metadata": {
        "id": "3DW7thWkzKx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "QGN23N_TzyF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have cuda/mps installed we will now be using the GPU.\n",
        "\n",
        "<br>\n",
        "\n",
        "Now that our device is defined, we need to move our computations to our GPU. This is done with `.to(device)`."
      ],
      "metadata": {
        "id": "HTX77dvfv_F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.ones(3, 3)\n",
        "print(f'Device upon initialization: {tensor1.device}')\n",
        "\n",
        "# moving the tensor to the gpu\n",
        "tensor1 = tensor1.to(device)\n",
        "print(f'Device after moving: {tensor1.device}')\n",
        "\n",
        "# we can also initialize devices upon tensor creation\n",
        "tensor2 = torch.ones(2, 2, device=device)\n",
        "print(f'Device upon initialization: {tensor2.device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45xE4c9rwYDD",
        "outputId": "aceafc7d-f078-4d9c-96c8-47efc09cdc30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device upon initialization: cpu\n",
            "Device after moving: cuda:0\n",
            "Device upon initialization: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all of our computations can be done significantly faster. Keep in mind that by default everything is created on the CPU and must manually be moved to the GPU. This includes models and loss calculations.\n",
        "\n",
        "<br>\n",
        "\n",
        "Lets see an example of how to move a model to our GPU."
      ],
      "metadata": {
        "id": "DNYqEV64xOv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class testModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(10, 5)\n",
        "    self.layer2 = nn.Linear(5, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.layer2(x)\n",
        "    return x\n",
        "\n",
        "model = testModel()\n",
        "print(f'Model device upon initialization: {model.layer1.weight.device}')\n",
        "model.to(device)\n",
        "print(f'Model device after moving: {model.layer1.weight.device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GFmvX5Xxcrz",
        "outputId": "1051fb01-bd0c-4286-dfb0-57f1af325e54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device upon initialization: cpu\n",
            "Model device after moving: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Note\n",
        "\n",
        "Cross device calculations cannot be completed. An error will always occur. Make sure that all tensors are on the same device before doing calculations. Always use `.to(device)` to make sure tensors are on the same device.\n",
        "\n",
        "Here is an example of an error you will get with cross device tensors."
      ],
      "metadata": {
        "id": "Se5aOdkp0dCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_tensor = torch.ones(3, 3)\n",
        "\n",
        "gpu_tensor = torch.ones(3, 3, device=device)\n",
        "\n",
        "try:\n",
        "  print(cpu_tensor + gpu_tensor)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poAmftwp0y9H",
        "outputId": "78392a6d-7d46-42cb-a33e-6376ab252394"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
          ]
        }
      ]
    }
  ]
}