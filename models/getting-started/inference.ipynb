{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-x906GCVK7K"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Colab link [here](https://colab.research.google.com/drive/16g7itVEzx2C6qVLkJu9KtgxzgdIciEJ8?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "In ML terms, inference is the act of a model making a prediction once it is fully trained. In this tutorial we are going to discuss how to properly do inference with your models.\n",
        "\n",
        "Inference is different than training in several aspects. It does not require gradients or parameter updates, purely a forward pass. If you've already seen my lesson on **evaluation** then you probably know what this means. We must use `model.eval()` and `torch.no_grad()` before sending any inputs.\n",
        "\n",
        "### Important Note\n",
        "\n",
        "During training, inputs to the model are always preprocessed. We must always use the same preprocessing steps during inference as well. Finally, if needed, the preprocessing steps are reversed.\n",
        "\n",
        "For example, if all the training images are normalized between [0,1] then the inference inputs must also be normalized between [0, 1].\n",
        "\n",
        "An example regarding LLM's is tokenization. Words are split into tokens during training. Since humans can't read tokens, the output is converted back to words.\n",
        "\n",
        "Here's a simple example of inference with a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAag0WmhU7QK"
      },
      "outputs": [],
      "source": [
        "# import necessary modules\n",
        "import torch\n",
        "\n",
        "# fake raw image\n",
        "image = torch.randn(1, 3, 50, 50) # batch dimension is included even in inference\n",
        "\n",
        "# preprocess image\n",
        "processed_image = preprocess(image)\n",
        "\n",
        "# code for model is omitted\n",
        "model = fakeModel()\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # forward pass only\n",
        "  output = model(processed_image)\n",
        "  prediction = output.argmax(1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
