{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d8b3ad",
   "metadata": {},
   "source": [
    "# What is deep learning?\n",
    "\n",
    "Everything in the universe is a function. Some functions are well known (how long it takes the earth to orbit the sun) while others are impossible for humans to wrap their heads around (the english languange). This is where deep learning comes in. By feeding a model inputs and the desired outputs it can learn to approximate an unknown function. Let's see the mechanics of how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e4793",
   "metadata": {},
   "source": [
    "***\n",
    "# The main deep learning topics we're covering\n",
    "\n",
    "- Models\n",
    "- Loss Functions\n",
    "- Activation Functions\n",
    "- Backpropagation\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fc02c",
   "metadata": {},
   "source": [
    "***\n",
    "# Models (Neural Network)\n",
    "\n",
    "### Perceptron\n",
    "A perceptron is the simplest part of a neural network. It takes a weighed sum of m inputs, adds a bias, and then multiplies them with a nonlinear activation function. \n",
    "\n",
    "$$\n",
    "y = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right) = f(\\mathbf{w} \\cdot \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "The perceptron can only classify between two linearaly separable classes. This is because an activation function for a single perceptron can return either 1 or 0.\n",
    "\n",
    "<img src=\"../images/perceptron.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "### Multi Layer Perceptron (MLP)\n",
    "What if we stack multiple of these perceptrons together? We get something called a feedforward network. An MLP can model much more complicated functions. Each perceptron's outputs are fed to the next layer's inputs. In an MLP, perceptrons are called neurons. The layers between the input and output are called hidden layers.\n",
    "\n",
    "<img src=\"../images/mlp.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f0024",
   "metadata": {},
   "source": [
    "***\n",
    "# Activation Functions\n",
    "\n",
    "Activation functions are nonlinear functions that allow linear equations from an MLP to model nonlinear functions. They also control the output range of a neuron. When training a model, activation functions must be differentiable so gradients can be computed during backpropagation.\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "The Sigmoid activation function is a function that ranges from (0, 1). It is used mainly for probabilities and commonly used as an output layer for binary classification. It suffers from a vanishing gradient issue.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "<img src=\"../images/sigmoid.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "### Tanh\n",
    "\n",
    "The Tanh activation function is similar to sigmoid but the output range is (-1, 1). Generally, it is better than sigmoid for hidden layers but still suffers from vanishing gradients.\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "<img src=\"../images/tanh.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "### ReLU\n",
    "\n",
    "ReLU is a special activation function. Its output range is [0, infinity). If you remember calculus, you may notice that this function is not differentiable at x = 0. To combat this we just define the derivative as either 1 or 0. ReLU is commonly used for hidden layers since it is simple to calculate. It suffers from the the dead neuron problem.\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "<img src=\"../images/relu.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is a modification of ReLU to fix the dying neuron problem. The output range is (-infinity, infinity). Rather than having all values <0 be 0, it defines a line with a very slight negative slope.\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & x \\ge 0 \\\\\n",
    "\\alpha x, & x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img src=\"../images/leaky_relu.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "### Softmax\n",
    "\n",
    "Unlike the other activation functions, this one cannot be used in hidden layers. Softmax creates a probability distribution from a vector of inputs, making it perfect for multi-class classification. The output range is (0, 1).\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd7014",
   "metadata": {},
   "source": [
    "*** \n",
    "# Loss Functions\n",
    "\n",
    "Loss functions tell a model how far its predictions are from the expected values. During training, the optimizer updates a model's weights to minimize the loss. Different tasks use different loss functions.\n",
    "\n",
    "Loss functions should be:\n",
    "- Differentiable\n",
    "- Suited for the training task\n",
    "\n",
    "### Mean Squared Error (MSE): Regression\n",
    "\n",
    "The most common loss for regression tasks. It penalizes larger errors more heavily than smaller errors due to squaring the error.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "### Mean Absolute Error (MAE): Regression\n",
    "\n",
    "Another common loss for regression tasks. It penalizes all errors equally.\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n \\left| \\hat{y}_i - y_i \\right|\n",
    "$$\n",
    "\n",
    "### Binary Cross-Entropy (BCE): Binary Classification\n",
    "\n",
    "A loss function used for classification between two classes.\n",
    "\n",
    "$$\n",
    "\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### Categorical Cross-Entropy: Multi-Class Classification\n",
    "\n",
    "A loss function for several classes. \n",
    "\n",
    "$$\n",
    "\\text{CCE} = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{c=1}^K y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4a4c2",
   "metadata": {},
   "source": [
    "***\n",
    "# Backpropagation\n",
    "\n",
    "This is the algorithm used to train neural networks. It computes the gradient of the loss function with respect to each weight in the model. These gradients are then used by an optimizer to update the weights and minimize the loss.\n",
    "Basically, backpropagation computes all partial derivatives of the weights in a model.\n",
    "\n",
    "$$\n",
    "\\text{partial derivative} = \\frac{\\partial L}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "<img src=\"../images/backpropagation.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde3124",
   "metadata": {},
   "source": [
    "***\n",
    "# Optimization\n",
    "\n",
    "Optimization is the process of adjusting a neural network's weights and biases to minimize the loss function. We are trying to find the global minimum of the loss function which can be thought of as a high dimensional surface. Using the gradients found from backpropagation, we decide how to tune the parameters.\n",
    "\n",
    "<img src=\"../images/gradient_descent.png\" \n",
    "        alt=\"Picture\" \n",
    "        height=400\n",
    "        width=600\n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Adam is the most common optimization function and the only one we will talk about here, it utilizes momentum and adaptive learning rates. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
